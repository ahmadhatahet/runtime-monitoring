{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Source:**<br />\n",
    "https://github.com/kuangliu/pytorch-cifar/blob/49b7aa97b0c12fe0d4054e670403a16b6b834ddd/models/dla_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST, FashionMNIST, GTSRB, Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ah19/runtime-monitoring')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path().cwd()\n",
    "\n",
    "if base.name != 'runtime-monitoring':\n",
    "    os.chdir('../')\n",
    "    base = Path().cwd()\n",
    "\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FLmxFGnxg6ZG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utils import load_json, load_pickle, get_dataset, get_models\n",
    "from utilities.pathManager import fetchPaths\n",
    "from utilities.pcaFunctions import applyPCASingle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mnist_model import MNIST_Model\n",
    "from models.fashionmnist_model import FashionMNIST_CNN\n",
    "from models.gtsrb_model import GTSRB_CNN\n",
    "from models.cifar10_dla import Cifar10_DLA\n",
    "from models.cifar10_model import Cifar10_CNN\n",
    "\n",
    "from models.transformers import transformers\n",
    "\n",
    "models = {\n",
    "    'mnist': MNIST_Model,\n",
    "    'fashionmnist': FashionMNIST_CNN,\n",
    "    'gtsrb': GTSRB_CNN,\n",
    "#     'cifar10': Cifar10_DLA,\n",
    "    'cifar10': Cifar10_CNN\n",
    "}\n",
    "\n",
    "\n",
    "targets_datasets = {\n",
    "    'MNIST': 'FashionMNIST',\n",
    "    'FashionMNIST': 'MNIST',\n",
    "    'GTSRB': 'Cifar10',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(DATASET):\n",
    "    paths = fetchPaths(Path().cwd(), DATASET, '', False)\n",
    "\n",
    "    configs = load_json(paths['configuration'])\n",
    "    config = configs['configuration']\n",
    "    optim = list(configs['configuration']['optimizer'].keys())[0]\n",
    "    batch_size = configs['model_config']['batch_size']\n",
    "\n",
    "    return config[\"dataset\"], config[\"lhl_neurons\"], config[\"flavors\"], optim, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1_000\n",
    "\n",
    "\n",
    "for base, target in targets_datasets.items():\n",
    "    DATASET, lhl_neurons, flavors, optim, batch_size = get_configs(base.lower())\n",
    "    \n",
    "    TARGET_DATASET = targets_datasets[DATASET]\n",
    "    \n",
    "    for FLAVOR in flavors:\n",
    "        for N in lhl_neurons:\n",
    "            \n",
    "            POSTFIX = f\"{optim}-{batch_size}-{N}\"\n",
    "            FILENAME_POSTFIX = f\"{DATASET}_{POSTFIX}\"\n",
    "            \n",
    "            # generate paths\n",
    "            base = Path().cwd()\n",
    "            paths = fetchPaths(base, DATASET, POSTFIX)\n",
    "            path_lhl = paths['lhl']\n",
    "            path_lhl_data = paths['lhl_' + FLAVOR]\n",
    "            path_bdd = paths['bdd'] / FLAVOR\n",
    "            path_bdd.mkdir(exist_ok=True)\n",
    "            path_target_data = paths['data'].parent / TARGET_DATASET\n",
    "            \n",
    "            # load pca and scaler objects\n",
    "            if FLAVOR == 'scaler_pca':\n",
    "                # load sacler object\n",
    "                sacler_ = load_pickle(path_lhl / f'scaler.pkl')\n",
    "                # load pca object\n",
    "                pca_ = load_pickle(path_lhl / f'scaler_pca.pkl')\n",
    "                \n",
    "                \n",
    "            # load config file\n",
    "            configs = load_json(paths['configuration'])\n",
    "            # model config\n",
    "            model_setup = configs['model_setup']\n",
    "            # saved model\n",
    "            path_model = next(paths['saved_models'].glob('*.pth.tar'))\n",
    "\n",
    "            # load trained model\n",
    "            model_ = models[DATASET.lower()]\n",
    "            transformer = transformers[DATASET.lower()]\n",
    "            \n",
    "            # load test data\n",
    "            test_data = get_dataset(TARGET_DATASET, path_target_data, train=False)\n",
    "\n",
    "            _, test_sample = train_test_split(\n",
    "                torch.range(0, len(test_data.targets)-1, dtype=torch.int32),\n",
    "                test_size=num_samples, shuffle=True, stratify=test_data.targets\n",
    "            )\n",
    "            # normalize data\n",
    "            x_target = transformer['test'].transforms[1](torch.stack([test_data[i][0] for i in test_sample]))\n",
    "\n",
    "\n",
    "            # torch 2.0 compile and parallel data training\n",
    "            model_setup['last_hidden_neurons'] = N\n",
    "            model = model_(**model_setup)\n",
    "            model = torch.compile(model)\n",
    "\n",
    "            # load model weights\n",
    "            model.load_state_dict(torch.load(path_model)['model'])\n",
    "\n",
    "            # eavluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # get last hidden layer logits for target data\n",
    "            logits_target, _ = model.output_last_layer(x_target)\n",
    "            logits_target = logits_target.numpy()\n",
    "            logits_target = pd.DataFrame(logits_target, columns=[f'x{i}' for i in range(logits_target.shape[1])])\n",
    "\n",
    "            # project data to pca\n",
    "            if FLAVOR == 'scaler_pca':\n",
    "                # transform data\n",
    "                logits_target = applyPCASingle(logits_target, sacler_, pca_, N)\n",
    "\n",
    "            # target data\n",
    "            df_logits_copy = logits_target.copy()\n",
    "            df_logits_copy['y'] = 1\n",
    "            df_logits_copy['true'] = 1\n",
    "            df_logits_copy.to_csv(path_lhl_data / f\"{FILENAME_POSTFIX}_{FLAVOR}_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mcDeYOA8d1wL"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b89e4666a1b1f5f90ada5928a1b04d9ad859bf674b23c56e0417352230a6456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
