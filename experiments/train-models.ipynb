{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST, FashionMNIST, GTSRB, Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'GTSRB'\n",
    "SEED = 42\n",
    "CUDA = 0\n",
    "GPU_NAME = f'cuda:{CUDA}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path().cwd()\n",
    "\n",
    "if base.name != 'runtime-monitoring':\n",
    "    os.chdir('../')\n",
    "    base = Path().cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLmxFGnxg6ZG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from fastprogress import progress_bar, master_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utils import *\n",
    "from utilities.pathManager import fetchPaths\n",
    "from utilities.scaleFunctions import *\n",
    "from utilities.pcaFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mnist_model import MNIST_Model\n",
    "from models.fashionmnist_model import FashionMNIST_CNN\n",
    "from models.gtsrb_model import GTSRB_CNN\n",
    "\n",
    "from models.transform import transform\n",
    "\n",
    "models = {\n",
    "    'mnist': MNIST_Model,\n",
    "    'fashionmnist': FashionMNIST_CNN,\n",
    "    'gtsrb': GTSRB_CNN\n",
    "}\n",
    "\n",
    "model_ = models[DATASET.lower()]\n",
    "transform = transform[DATASET.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = fetchPaths(base, DATASET, '', False)\n",
    "\n",
    "path_data = paths['data']\n",
    "path_lhl = paths['lhl']\n",
    "path_stats = paths['saved_models'].parent.parent\n",
    "\n",
    "configs = load_json(paths['configuration'])\n",
    "config = configs['configuration']\n",
    "model_setup = configs['model_setup']\n",
    "model_config = configs['model_config']\n",
    "optim_name = list(config['optimizer'].keys())[0]\n",
    "optim_args = config['optimizer'][optim_name]\n",
    "scheduler_name = list(config['scheduler'].keys())[0]\n",
    "scheduler_args = config['scheduler'][scheduler_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7eERD3yVH2H"
   },
   "source": [
    "# GPU Device & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS9DVaKDi_2C"
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jRXv8RAhzqa",
    "outputId": "d8ccff46-cebe-4890-ab1c-5b498a395e64"
   },
   "outputs": [],
   "source": [
    "device = get_device(GPU_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7eERD3yVH2H"
   },
   "source": [
    "# Load / Split / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_labels(DATASET)\n",
    "\n",
    "train_data = get_dataset(DATASET, path_data, train=True, transform=transform['train'])\n",
    "test_data = get_dataset(DATASET, path_data, train=False, transform=transform['test'])\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94CFn70JlO-P"
   },
   "outputs": [],
   "source": [
    "trainloader = get_dataLoader(train_data, model_config['batch_size'], True)\n",
    "testloader = get_dataLoader(test_data, model_config['batch_size'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_denormalize = T.Normalize(\n",
    "    mean=[-m / s for m, s in zip(transform['train'].transforms[-1].mean, transform['train'].transforms[-1].std)],\n",
    "    std=[1/s for s in transform['train'].transforms[-1].std]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mean_std(DataLoader(train_data, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_images_loader(trainloader, feature_names=feature_names, transform=tf_denormalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lhl):\n",
    "    \n",
    "    # model\n",
    "    model_setup['last_hidden_neurons'] = lhl\n",
    "    \n",
    "    # torch 2.0 compile and parallel data training\n",
    "    model = model_(**model_setup).to(device)\n",
    "    model = torch.compile(model)\n",
    "    nn.DataParallel(model, device_ids=[CUDA])\n",
    "    \n",
    "    # loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # optimizer and scheduler\n",
    "    optimizer = getattr(torch.optim, optim_name)(model.parameters(), lr=model_config['lr'], **optim_args)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, scheduler_name)(optimizer, **scheduler_args)\n",
    "        \n",
    "    return model, loss_function, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(postfix):\n",
    "    model_name = f'{DATASET}_{postfix}'\n",
    "    paths = fetchPaths(base, DATASET, postfix)\n",
    "    path_saved_model = paths['saved_models']\n",
    "    path_lhl_raw = paths['lhl_raw']\n",
    "    \n",
    "    return model_name, path_saved_model, path_lhl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training_testing(model_name, model, loss_function, optimizer, scheduler):\n",
    "    # training testing attributes\n",
    "    kwargs = {\n",
    "        'model': model,\n",
    "        'loss_function': loss_function,\n",
    "        'optimizer': optimizer,\n",
    "        'lr_scheduler': scheduler,\n",
    "        'device': device,\n",
    "        'model_path': path_saved_model / f\"{model_name}.pth.tar\",\n",
    "        'trainloader': trainloader,\n",
    "        'testloader': testloader,\n",
    "        'config': model_config\n",
    "    }\n",
    "\n",
    "    # run training testing\n",
    "    return run_training_testing(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_stats = pd.DataFrame({\n",
    "    'lhl':pd.Series(dtype=str),\n",
    "    'optim':pd.Series(dtype=str),\n",
    "    'scheduler':pd.Series(dtype=str),\n",
    "    'epochs':pd.Series(dtype=np.uint8),\n",
    "    'best_epoch':pd.Series(dtype=np.uint8),\n",
    "    'train_losses':pd.Series(dtype=object),\n",
    "    'test_losses':pd.Series(dtype=object),\n",
    "    'train_accs':pd.Series(dtype=object),\n",
    "    'test_accs':pd.Series(dtype=object),\n",
    "    'test_loss':pd.Series(dtype=np.float32),\n",
    "    'test_acc':pd.Series(dtype=np.float32)\n",
    "})\n",
    "\n",
    "for lhl in config['lhl_neurons']:\n",
    "\n",
    "    # model postfix\n",
    "    postfix = f\"{optim_name}-{model_config['batch_size']}-{lhl}\"\n",
    "    model_name, path_saved_model, path_lhl_raw = create_folders(postfix)\n",
    "    \n",
    "    # create model\n",
    "    model, loss_function, optimizer, scheduler = create_model(lhl)\n",
    "    \n",
    "    # skip train and only load if exists\n",
    "    skip_train = len(list(path_saved_model.glob(f'{model_name}*.pth.tar'))) != 0\n",
    "    \n",
    "    if skip_train: continue\n",
    "        \n",
    "    # train\n",
    "    (train_losses, train_accs,\n",
    "    test_losses, test_accs,\n",
    "    train_loss, train_acc,\n",
    "    test_loss, test_acc,\n",
    "    confusion_matrix_train,\n",
    "    confusion_matrix_test,\n",
    "    model_path) = start_training_testing(model_name, model, loss_function, optimizer, scheduler)\n",
    "    \n",
    "    # save stats\n",
    "    model_stats.loc[model_stats.shape[0]+1] = [lhl, optim_name, scheduler_name, len(train_losses), np.argmax(test_accs), train_losses, test_losses, train_accs, test_accs, test_loss, test_acc]\n",
    "\n",
    "    # load best model\n",
    "    load_checkpoint(model, best_model_name)\n",
    "    \n",
    "    # normalize and save matrix\n",
    "    confusion_matrix_test_norm = normalize_confusion_matrix(confusion_matrix_test)\n",
    "    save_confusion_matrix(confusion_matrix_test_norm, path_saved_model, model_name, 'test')\n",
    "    \n",
    "    # export last hidden layer data\n",
    "    export_last_hidden_layer(trainloader, model, device, lhl, path_lhl_raw, model_name, 'raw_train')\n",
    "    export_last_hidden_layer(testloader, model, device, lhl, path_lhl_raw, model_name, 'raw_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model stats csv\n",
    "model_stats.to_csv(path_stats / f'{DATASET}_model_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lhl in config['lhl_neurons']:\n",
    "    \n",
    "    # model postfix\n",
    "    postfix = f\"{optim_name}-{model_config['batch_size']}-{lhl}\"\n",
    "    model_name = f\"{DATASET}_{postfix}\"\n",
    "\n",
    "    # get paths\n",
    "    paths_ = fetchPaths(base, DATASET, postfix)\n",
    "    p_lhl = paths_['lhl']\n",
    "    p_lhl_raw = paths_['lhl_raw']\n",
    "    p_lhl_pca = paths_['lhl_pca']\n",
    "    \n",
    "    # load data\n",
    "    train = pd.read_csv(p_lhl_raw / f'{model_name}_raw_train.csv')\n",
    "    true = train.loc[train['true'] == True]\n",
    "    test = pd.read_csv(p_lhl_raw / f'{model_name}_raw_test.csv')\n",
    "    \n",
    "    # fit scaler and pca\n",
    "    scaler_ = fitStandardScalerSingle(true, lhl)\n",
    "    pca_ = fitPCASingle(true, scaler=scaler_, numNeurons=lhl)\n",
    "    \n",
    "    # save objects\n",
    "    save_pickle(p_lhl / 'scaler.pkl', scaler_)\n",
    "    save_pickle(p_lhl / 'pca.pkl', pca_)\n",
    "\n",
    "    # transform and save data\n",
    "    ## train\n",
    "    applyPCASingle(p_lhl_pca, train, scaler=scaler_, numNeurons=lhl).to_csv(p_lhl_pca / f'{model_name}_pca_train.csv', index=False)\n",
    "    \n",
    "    ## test\n",
    "    applyPCASingle(p_lhl_pca, test, scaler=scaler_, numNeurons=lhl).to_csv(p_lhl_pca / f'{model_name}_pca_test.csv', index=False)\n",
    "    \n",
    "    # save selected neurons\n",
    "    gte_mean, top_third = neuronsLoadingsSingle(pca_, numNeurons=lhl, var_thld=0.9, loadings_thld=0.2)\n",
    "    save_json(p_lhl / 'neurons_gte_mean.json', gte_mean)\n",
    "    save_json(p_lhl / 'neurons_top_third.json', top_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mcDeYOA8d1wL"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b89e4666a1b1f5f90ada5928a1b04d9ad859bf674b23c56e0417352230a6456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
