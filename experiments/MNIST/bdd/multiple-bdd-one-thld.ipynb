{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env variables\n",
    "PREFIX = 'Adam-256-30'\n",
    "FILENAME_POSTFIX = f'{DATASET}_{PREFIX}'\n",
    "FALVOR = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7389b95b-0d7b-40a7-ad0a-0d6cb25c3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "/home/ah19/runtime-monitoring\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "base = Path().cwd()\n",
    "# switch to home directory to import helper scripts\n",
    "if str(base).split('/')[-1] != 'runtime-monitoring':\n",
    "    DATASET = str(base).split('/')[5]\n",
    "    os.chdir('../../..')\n",
    "    base = Path().cwd()\n",
    "\n",
    "print(DATASET)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe97988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import env virables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(base / 'configurations' / 'thresholds.env')\n",
    "\n",
    "# import helper functions\n",
    "from utilities.utils import load_json\n",
    "from utilities.pathManager import fetchPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7f5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pympler import asizeof\n",
    "import time\n",
    "from pathlib import Path\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e1c295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = fetchPaths(base, DATASET, PREFIX)\n",
    "\n",
    "path_data = paths['data']\n",
    "path_model = paths['model']\n",
    "path_saved_models = paths['saved_models']\n",
    "path_bdd = paths['bdd']\n",
    "path_lhl = paths['lhl_' + FALVOR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae325d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train Data ...\n",
      "Loading test Data ...\n",
      "Loading Neurons ...\n"
     ]
    }
   ],
   "source": [
    "print('Loading train Data ...')\n",
    "df = pd.read_csv(path_lhl / f'{FILENAME_POSTFIX}_train.csv')\n",
    "\n",
    "print('Loading test Data ...')\n",
    "df_test = pd.read_csv(path_lhl / f'{FILENAME_POSTFIX}_test.csv')\n",
    "\n",
    "print('Loading Neurons ...')\n",
    "neurons_ = load_json(paths['lhl_pca'] / f'{FILENAME_POSTFIX}_neurons.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8b16178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from dd.autoref import BDD\n",
    "\n",
    "def declare_vars(bdd_):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # generate vars either x0_0 or x0_0 and x0_1 per neuron\n",
    "    vars_range = neurons if neurons is not None else range(num_neurons)\n",
    "    v = [f'x{n}' for n in vars_range]\n",
    "\n",
    "    # add vars to bdd\n",
    "    [ *map(bdd_.add_var, v) ]\n",
    "\n",
    "    # generate negative vars\n",
    "    vars = np.array([ *map(bdd_.var, v) ])\n",
    "    vars_not = np.array([ ~v for v in vars ])\n",
    "\n",
    "    return vars, vars_not\n",
    "\n",
    "\n",
    "def applying_thlds(df):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    df_thld = (df >=  thld).astype('int8')\n",
    "    return df_thld.to_numpy()\n",
    "\n",
    "\n",
    "def construct_pattern(row):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # replace 1 with vars and 0 with vars_not\n",
    "    expr = np.where( row == 1, self.vars, self.vars_not )\n",
    "    return np.bitwise_and.reduce( expr )\n",
    "\n",
    "\n",
    "def construct_pattern_parallel(args):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    vars_, vars_not_, row = args\n",
    "    # replace 1 with vars and 0 with vars_not\n",
    "    expr = np.where( row == 1, vars_, vars_not_ )\n",
    "    return np.bitwise_and.reduce( expr )\n",
    "\n",
    "    \n",
    "\n",
    "def check_one_pattern(row):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    if (roots & self.construct_pattern(row) ) == self.bdd.false:\n",
    "        return 0 # means not found\n",
    "    else:\n",
    "        return 1 # found it\n",
    "\n",
    "\n",
    "def score_dataframe(df, bdd_col='bdd'):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    df_all_classes = df[['y', 'true']].groupby('y').count().sort_index()\n",
    "    df_all_classes.columns = ['count']\n",
    "\n",
    "    df_out_of_pattern_images = df.loc[df[bdd_col] == 0, ['y', bdd_col]].groupby('y').count().sort_index()\n",
    "    df_out_of_pattern_images.columns = [bdd_col + '_false']\n",
    "\n",
    "    df_out_of_pattern_misclassified_images = df.loc[(df[bdd_col] == 0) & (df['true'] == False), ['y', bdd_col]].groupby('y').count().sort_index()\n",
    "    df_out_of_pattern_misclassified_images.columns = [bdd_col + '_false_miss_classified']\n",
    "\n",
    "    df_scores = df_all_classes.join(df_out_of_pattern_images).join(df_out_of_pattern_misclassified_images)\n",
    "\n",
    "    del df_out_of_pattern_images, df_out_of_pattern_misclassified_images\n",
    "\n",
    "    total_images = df_all_classes['count'].sum()\n",
    "    out_of_pattern_images = (df[bdd_col] == 0).sum()\n",
    "    out_of_pattern_misclassified_images = ((df['true'] == False) & (df[bdd_col] == 0)).sum()\n",
    "    df_scores.loc['all', :] = [total_images, out_of_pattern_images, out_of_pattern_misclassified_images]\n",
    "\n",
    "    # if data frame return 0 rows, a nan will be placed\n",
    "    df_scores.fillna(0, inplace=True)\n",
    "\n",
    "    # calculate metrics\n",
    "    df_scores['outOfPattern'] = df_scores[bdd_col + '_false'] / df_scores['count']\n",
    "    df_scores['outOfPatternMissClassified'] = df_scores[bdd_col + '_false_miss_classified'] / df_scores[bdd_col + '_false']\n",
    "\n",
    "    # add mean of all classes\n",
    "    a1 = df_scores.loc[df_scores.index != 'all', 'outOfPattern'].mean()\n",
    "    a2 = df_scores.loc[df_scores.index != 'all', 'outOfPatternMissClassified'].mean()\n",
    "    df_scores.loc['all_mean', :] = [0, 0, total_images, a1, a2]\n",
    "\n",
    "    # if class is never missclassified and bdd recognize all of his patterns\n",
    "    # both outOfPattern and outOfPatternMissClassified will be 0\n",
    "    # so the division will result in NaN\n",
    "    df_scores['outOfPatternMissClassified'].replace({np.nan:0.0, 0.0:1.0}, inplace=True)\n",
    "    df_scores['outOfPattern'].replace({np.nan:0.0}, inplace=True)\n",
    "\n",
    "    # no missclassification for a class\n",
    "    df_scores[bdd_col + '_false'].replace({np.nan:0.0}, inplace=True)\n",
    "    df_scores[bdd_col + '_false_miss_classified'].replace({np.nan:0.0}, inplace=True)\n",
    "\n",
    "    if bdd_col=='bdd':\n",
    "        return df_scores.reset_index()\n",
    "    return df_scores\n",
    "\n",
    "\n",
    "def evaluate_dataframe(self, df, eta=None):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    bdd_results = np.zeros(df.shape[0], dtype=np.int8)\n",
    "\n",
    "    if self.neurons is not None:\n",
    "        patterns = applying_thlds(df[df.columns[self.neurons]])\n",
    "    else:\n",
    "        patterns = applying_thlds(df[df.columns[:self.num_neurons]])\n",
    "\n",
    "    bdd_results = np.apply_along_axis(self.check_one_pattern, 1, patterns)\n",
    "\n",
    "    # if the function called specifically, return scored df after evaluating\n",
    "    df['bdd'] = bdd_results\n",
    "\n",
    "    return score_dataframe(df)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_dataframe(self, df, eta=None):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    bdd_results = np.zeros(df.shape[0], dtype=np.int8)\n",
    "\n",
    "    if self.neurons is not None:\n",
    "        patterns = applying_thlds(df[df.columns[self.neurons]])\n",
    "    else:\n",
    "        patterns = applying_thlds(df[df.columns[:self.num_neurons]])\n",
    "\n",
    "    if self.num_bits == 2:\n",
    "        patterns = np.apply_along_axis(multi_thlds, 1, patterns)\n",
    "\n",
    "    bdd_results = np.apply_along_axis(check_one_pattern, 1, patterns)\n",
    "\n",
    "    if eta is not None:\n",
    "        df[f'bdd_{eta}'] = bdd_results\n",
    "        return\n",
    "\n",
    "    # if the function called specifically, return scored df after evaluating\n",
    "    df['bdd'] = bdd_results\n",
    "\n",
    "    return score_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4a23e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct bdds ..\n",
      "start pool ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-19-3d5498b22d92>\", line 42, in add_patterns\n    or_expressions = np.apply_along_axis(construct_pattern_parallel, 1, (vars_, vars_not_, pattern))\n  File \"<__array_function__ internals>\", line 6, in apply_along_axis\n  File \"/usr/local/lib64/python3.6/site-packages/numpy/lib/shape_base.py\", line 361, in apply_along_axis\n    axis = normalize_axis_index(axis, nd)\nnumpy.AxisError: axis 1 is out of bounds for array of dimension 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3d5498b22d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     results = pool.map(add_patterns, [(root_, vars_, vars_not_, pattern)\n\u001b[0;32m---> 73\u001b[0;31m                                     for (root_, (vars_, vars_not_), pattern) in zip(roots, vars, df_true_patterns)])\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish pool ..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# split train data\n",
    "df_true = df[df['true'] == True].copy()\n",
    "df_true = df_true.drop('true', axis=1).reset_index(drop=True)\n",
    "\n",
    "# define threshold\n",
    "p = 0.95\n",
    "\n",
    "thld = np.quantile(df_true.drop('y', axis=1), p, axis=0)\n",
    "thld_name = f'qth_{p}'\n",
    "\n",
    "# degree of freedom\n",
    "eta = 0\n",
    "\n",
    "\n",
    "bdd_bdd = BDD()\n",
    "bdd_roots = bdd.false\n",
    "n_jobs = 5\n",
    "num_neurons = 30\n",
    "neurons = None\n",
    "if neurons_ is not None:\n",
    "    neurons = [int(x[1:]) for x in neurons_]\n",
    "    thld = thld[neurons]\n",
    "\n",
    "bdd_vars, bdd_vars_not = declare_vars(bdd_bdd)\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    'thld': [],\n",
    "    'df_true': [],\n",
    "    'build_time': [],\n",
    "    'size_before_reorder_mb': [],\n",
    "    'reorder_time': [],\n",
    "    'size_after_reorder_mb': []\n",
    "})\n",
    "    \n",
    "\n",
    "eval_df_trues=[df_true.copy(), df_test.copy()]    \n",
    "    \n",
    "def add_patterns(args):\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    (root_, vars_, vars_not_, pattern) = args\n",
    "    or_expressions = np.apply_along_axis(construct_pattern_parallel, 1, (vars_, vars_not_, pattern))\n",
    "    root_ |= np.bitwise_or.reduce( or_expressions )\n",
    "\n",
    "    return root_\n",
    "\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "if neurons is not None:\n",
    "    df_true = df_true[df_true.columns[neurons]].drop_duplicates()\n",
    "else:\n",
    "    df_true = df_true[df_true.columns[:num_neurons]].drop_duplicates()\n",
    "\n",
    "df_true_pattern = applying_thlds(df_true)\n",
    "\n",
    "# START POOL\n",
    "num_procs = mp.cpu_count() if n_jobs == -1 else n_jobs\n",
    "\n",
    "# initiate bdds and vars\n",
    "print('construct bdds ..')\n",
    "\n",
    "bdds = [BDD() for i in range(num_procs)]\n",
    "roots = [b.false for b in bdds]\n",
    "vars = [declare_vars(b) for b in bdds]\n",
    "df_true_patterns = np.array_split(df_true_pattern, num_procs)\n",
    "\n",
    "print('start pool ..')\n",
    "with mp.Pool(num_procs) as pool:\n",
    "\n",
    "    results = pool.map(add_patterns, [(root_, vars_, vars_not_, pattern)\n",
    "                                    for (root_, (vars_, vars_not_), pattern) in zip(roots, vars, df_true_patterns)])\n",
    "\n",
    "print('finish pool ..')\n",
    "# join bdds\n",
    "for r in results:\n",
    "    bdd_roots |= r\n",
    "print('joined bdds ..')\n",
    "\n",
    "build_time = round(time.perf_counter() - start, 3)\n",
    "\n",
    "\n",
    "row = self.stats.shape[0]+1\n",
    "self.stats.loc[row, 'df_true'] = 0\n",
    "self.stats.loc[row, 'build_time'] = build_time\n",
    "self.stats.loc[row, 'size_before_reorder_mb'] = round( asizeof.asizeof(self) * 1e-6, 3)\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "BDD.reorder(bdd_bdd)\n",
    "bdd_reorder_time = round(time.perf_counter() - start, 3)\n",
    "\n",
    "\n",
    "stats.loc[row, 'reorder_time'] = bdd_reorder_time\n",
    "stats.loc[row, 'size_after_reorder_mb'] = round( asizeof.asizeof(self) * 1e-6, 3)\n",
    "\n",
    "\n",
    "# add column for scoring\n",
    "if eval_df_trues is not None:\n",
    "    for eval_df_true in eval_df_trues:\n",
    "        evaluate_dataframe(eval_df_true, 0)\n",
    "\n",
    "stats = stats.loc[stats['df_true'] == eta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa7f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
