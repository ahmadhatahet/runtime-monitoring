{
    "configuration": {
        "dataset": "GTSRB",
        "optimizer": {
            "AdamW": {}
        },
        "scheduler": {
            "MultiStepLR": {
                "milestones": [5, 20]
            }
        },
        "lhl_neurons": [
            30,
            50,
            80,
            150
        ],
        "flavors": [
            "raw",
            "scaler_pca"
        ],
        "subset_neurons": [
            1,
            0
        ],
        "eta": 0
    },
    "model_setup": {
        "dropout": 0.25
    },
    "model_config": {
        "batch_size": 32,
        "lr": 0.01,
        "epochs": 25,
        "patience": 10,
        "L2": 1e-5,
        "L1": 1e-5
    }
}